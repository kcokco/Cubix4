import json
import os
from typing import Dict, List, Any
from openai import OpenAI
from dotenv import load_dotenv
from response_generator import full_response_pipeline
from vectordb import VectorDB

# Load environment variables
load_dotenv()

CORRECTNESS_JUDGE_PROMPT = """
You are an expert evaluator. Your task is to evaluate whether a generated response is correct by comparing it with the ground truth answer.

Ground Truth: {ground_truth}
Generated Response: {generated_response}

Evaluate if the generated response is factually correct and contains the same information as the ground truth.
Consider the response correct if:
1. It contains the key information from the ground truth
2. It doesn't contradict the ground truth
3. It provides accurate information even if phrased differently

Please provide your evaluation in the following format:
REASONING: [Detailed explanation of your decision, including what key information was present/missing, any contradictions found, and how the generated response compares to the ground truth. Show concrete examples from the ground truth and the generated response. If something is missing, show it from the ground truth. If something is not in the ground truth, then show it from the generated response.]
DECISION: [CORRECT or INCORRECT]
"""

RELEVANCE_JUDGE_PROMPT = """
You are an expert evaluator. Your task is to evaluate whether a generated response is relevant to the user query.

User Query: {query}
Generated Response: {generated_response}

Evaluate if the generated response directly addresses the user's question and provides relevant information.
Consider the response relevant if:
1. It directly answers the question asked
2. It provides information that helps the user with their query
3. It stays on topic and doesn't go off on tangents

Please provide your evaluation in the following format:
REASONING: [Detailed explanation of your decision, including how well the response addresses the query, what aspects are relevant/irrelevant, and whether it provides helpful information to the user]
DECISION: [RELEVANT or IRRELEVANT]
"""

def evaluate_correctness(generated_response: str, ground_truth: str) -> Dict[str, Any]:
    """
    Evaluate if the generated response is correct compared to ground truth.
    
    Args:
        generated_response: The response generated by the RAG system
        ground_truth: The expected correct response
    
    Returns:
        Dictionary with evaluation result and explanation
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OpenAI API key must be provided as OPENAI_API_KEY environment variable")
    
    client = OpenAI(api_key=api_key)
    
    prompt = CORRECTNESS_JUDGE_PROMPT.format(
        ground_truth=ground_truth,
        generated_response=generated_response
    )
    
    try:
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[{"role": "user", "content": prompt}]
        )
        
        result = response.choices[0].message.content.strip()
        
        # Parse structured response
        decision = None
        reasoning = None
        
        lines = result.split('\n')
        for line in lines:
            if line.startswith("DECISION:"):
                decision = line.replace("DECISION:", "").strip()
            elif line.startswith("REASONING:"):
                reasoning = line.replace("REASONING:", "").strip()
        
        # Fallback parsing if structured format not followed
        if decision is None:
            is_correct = "CORRECT" in result.upper()
            decision = "CORRECT" if is_correct else "INCORRECT"
        else:
            is_correct = decision.upper() == "CORRECT"
        
        if reasoning is None:
            reasoning = result
        
        return {
            "is_correct": is_correct,
            "decision": decision,
            "reasoning": reasoning,
            "explanation": result,
            "raw_response": result
        }
    
    except Exception as e:
        return {
            "is_correct": False,
            "explanation": f"Error in evaluation: {str(e)}",
            "raw_response": None
        }

def evaluate_relevance(generated_response: str, query: str) -> Dict[str, Any]:
    """
    Evaluate if the generated response is relevant to the user query.
    
    Args:
        generated_response: The response generated by the RAG system
        query: The user's original query
    
    Returns:
        Dictionary with evaluation result and explanation
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OpenAI API key must be provided as OPENAI_API_KEY environment variable")
    
    client = OpenAI(api_key=api_key)
    
    prompt = RELEVANCE_JUDGE_PROMPT.format(
        query=query,
        generated_response=generated_response
    )
    
    try:
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[{"role": "user", "content": prompt}]
        )
        
        result = response.choices[0].message.content.strip()
        
        # Parse structured response
        decision = None
        reasoning = None
        
        lines = result.split('\n')
        for line in lines:
            if line.startswith("DECISION:"):
                decision = line.replace("DECISION:", "").strip()
            elif line.startswith("REASONING:"):
                reasoning = line.replace("REASONING:", "").strip()
        
        # Fallback parsing if structured format not followed
        if decision is None:
            is_relevant = "RELEVANT" in result.upper() and "IRRELEVANT" not in result.upper()
            decision = "RELEVANT" if is_relevant else "IRRELEVANT"
        else:
            is_relevant = decision.upper() == "RELEVANT"
        
        if reasoning is None:
            reasoning = result
        
        return {
            "is_relevant": is_relevant,
            "decision": decision,
            "reasoning": reasoning,
            "explanation": result,
            "raw_response": result
        }
    
    except Exception as e:
        return {
            "is_relevant": False,
            "explanation": f"Error in evaluation: {str(e)}",
            "raw_response": None
        }

def load_golden_dataset(file_path: str) -> Dict[str, Any]:
    """
    Load the golden dataset from JSON file.
    
    Args:
        file_path: Path to the golden dataset JSON file
    
    Returns:
        Loaded dataset dictionary
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def evaluate_single_turn(vector_db: VectorDB, golden_dataset_path: str) -> Dict[str, Any]:
    """
    Perform single turn evaluation on the RAG system using the golden dataset.
    
    Args:
        vector_db: VectorDB instance for document retrieval
        golden_dataset_path: Path to the golden dataset JSON file
    
    Returns:
        Evaluation results with metrics and detailed results
    """
    # Load golden dataset
    dataset = load_golden_dataset(golden_dataset_path)
    
    results = {
        "metadata": dataset["metadata"],
        "total_queries": 0,
        "correct_responses": 0,
        "relevant_responses": 0,
        "accuracy": 0.0,
        "relevance_rate": 0.0,
        "detailed_results": []
    }
    
    # Process each entry in the dataset
    for entry in dataset["entries"]:
        document_info = entry["document"]
        qa_pairs = entry["qa_pairs"]
        
        for qa_pair in qa_pairs:
            query = qa_pair["query"]
            ground_truth = qa_pair["response"]
            
            # Generate response using RAG pipeline
            generated_response = full_response_pipeline(query, vector_db)
            
            # Evaluate correctness
            correctness_eval = evaluate_correctness(generated_response, ground_truth)
            
            # Evaluate relevance
            relevance_eval = evaluate_relevance(generated_response, query)
            
            # Store detailed results
            detailed_result = {
                "document_title": document_info["title"],
                "document_file": document_info["file_path"],
                "query": query,
                "ground_truth": ground_truth,
                "generated_response": generated_response,
                "correctness": correctness_eval,
                "relevance": relevance_eval
            }
            
            results["detailed_results"].append(detailed_result)
            
            # Update counters
            results["total_queries"] += 1
            if correctness_eval["is_correct"]:
                results["correct_responses"] += 1
            if relevance_eval["is_relevant"]:
                results["relevant_responses"] += 1
    
    # Calculate final metrics
    if results["total_queries"] > 0:
        results["accuracy"] = results["correct_responses"] / results["total_queries"]
        results["relevance_rate"] = results["relevant_responses"] / results["total_queries"]
    
    return results

def save_evaluation_results(results: Dict[str, Any], output_path: str):
    """
    Save evaluation results to a JSON file.
    
    Args:
        results: Evaluation results dictionary
        output_path: Path to save the results
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

def print_evaluation_summary(results: Dict[str, Any]):
    """
    Print a summary of the evaluation results.
    
    Args:
        results: Evaluation results dictionary
    """
    print("="*60)
    print("SINGLE TURN EVALUATION RESULTS")
    print("="*60)
    print(f"Total Queries: {results['total_queries']}")
    print(f"Correct Responses: {results['correct_responses']}")
    print(f"Relevant Responses: {results['relevant_responses']}")
    print(f"Accuracy: {results['accuracy']:.2%}")
    print(f"Relevance Rate: {results['relevance_rate']:.2%}")
    print("="*60)
    
    # Print some example results
    print("\nSample Results:")
    for i, result in enumerate(results["detailed_results"][:3]):
        print(f"\n--- Example {i+1} ---")
        print(f"Query: {result['query']}")
        print(f"Generated: {result['generated_response'][:100]}...")
        print(f"Correct: {result['correctness']['is_correct']}")
        print(f"Relevant: {result['relevance']['is_relevant']}")

if __name__ == "__main__":
    # Initialize VectorDB
    vector_db = VectorDB()
    
    # Path to golden dataset
    golden_dataset_path = "golden_dataset_v1.2.x_20250922_103806.json"
    
    # Run evaluation
    print("Starting single turn evaluation...")
    results = evaluate_single_turn(vector_db, golden_dataset_path)
    
    # Create output directory if it doesn't exist
    os.makedirs("output", exist_ok=True)
    
    # Save results
    output_path = "output/single_turn_evaluation_results_gpt-5-full.json"
    save_evaluation_results(results, output_path)
    
    # Print summary
    print_evaluation_summary(results)
    
    print(f"\nDetailed results saved to: {output_path}")